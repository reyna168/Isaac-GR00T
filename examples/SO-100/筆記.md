## OpenAI 解釋


好的 🚀 我來逐段解說這份 SO100 實體機械手臂控制程式，讓你清楚知道每個部分的用途。這份程式主要是用來 控制 SO100 機械臂，並且可以選擇：

使用訓練好的 Policy（AI 推論） → 即時執行語言指令（例如「把水果放到盤子裡」）。

重播資料集 → 從 Hugging Face 上的動作資料集（so100_strawberry_grape）重現動作。

🏗️ 程式架構

主要分成三大部分：

SO100Robot 類別 → 機械臂的封裝與控制。

Gr00tRobotInferenceClient 類別 → 與外部推理服務（Policy Server）溝通。

Main 主程式 → 選擇要跑 Policy 模式或 Dataset Replay 模式。

🔹 1. SO100Robot 類別

負責機械臂本體的連線、控制、初始化與關機。

初始化
class SO100Robot:
    def __init__(self, calibrate=False, enable_camera=False, cam_idx=9):
        self.config = So100RobotConfig()


calibrate: 是否重新校正（會刪掉 .cache/calibration/so100）

enable_camera: 是否啟用攝影機

cam_idx: 指定攝影機 index

它會建立 So100RobotConfig，決定是否啟動相機，並用 make_robot_from_config() 生成 robot 物件。

connect()
def connect(self):
    self.motor_bus.connect()
    self.motor_bus.write("Torque_Enable", TorqueMode.DISABLED.value)
    self.robot.activate_calibration()
    self.set_so100_robot_preset()
    self.motor_bus.write("Torque_Enable", TorqueMode.ENABLED.value)


連線到馬達控制總線

關閉扭力 → 校正位置

設定機械臂的 控制模式與 PID（避免晃動，提升加速度）

開啟扭力，進入可控狀態

同時會啟用相機（如果 enable_camera=True）。

set_so100_robot_preset()

設定機械臂的控制參數：

Mode = 0 → 位置控制模式

P_Coefficient = 10（降低震動）

Acceleration = 254（加速反應）

這些參數是針對 Dynamixel 馬達的配置。

move_to_initial_pose()

讓機械臂移動到預設的初始姿勢：

current_state = torch.tensor([90, 90, 90, 90, -70, 30])
self.robot.send_action(current_state)

go_home()

移動到「home 姿勢」，用一組固定的關節角度。

觀測與控制

get_observation() → 擷取機械臂的完整觀測資料（狀態+影像）。

get_current_state() → 取得當前的 6 維關節狀態（肩膀、手肘、手腕、夾爪）。

get_current_img() → 拍攝即時相機影像。

set_target_state() → 發送動作到機械臂。

disconnect()

斷開機械臂連線並釋放馬達扭力。

🔹 2. Gr00tRobotInferenceClient 類別

負責和 外部推理服務（ExternalRobotInferenceClient）溝通，將 影像 + 機械臂狀態 + 語言指令 發送給 AI Policy，並接收動作。

obs_dict = {
    "video.webcam": img[np.newaxis, :, :, :],
    "state.single_arm": state[:5][np.newaxis, :].astype(np.float64),
    "state.gripper": state[5:6][np.newaxis, :].astype(np.float64),
    "annotation.human.task_description": [self.language_instruction],
}
res = self.policy.get_action(obs_dict)


video.webcam → 影像輸入

state.single_arm → 機械臂 5 維關節狀態

state.gripper → 夾爪狀態

annotation.human.task_description → 語言指令（例如「Pick up the fruits...」）

get_action() → 輸出 Policy 的動作序列
smaple_action() → 測試用，輸入零向量，拿到一個隨機動作

🔹 3. Main 主程式

進入點，解析參數：

python so100_eval.py \
    --use_policy \
    --host 10.110.17.183 \
    --port 5555 \
    --lang_instruction "Pick up the fruits and place them on the plate."

兩種模式：
(A) Policy 模式

建立 Gr00tRobotInferenceClient

啟動 SO100Robot

進入迴圈：

拍攝相機畫面 + 讀取狀態

丟到 Policy → 得到動作序列

依序執行每個動作，驅動馬達

可選擇紀錄影像

(B) Dataset Replay 模式

載入 Hugging Face 上的 so100_strawberry_grape dataset

讀取動作 & 對應影像

將資料集中的動作依序執行在真實機械臂上

並將影像與即時攝影畫面進行對照

最後會畫出整段的動作序列圖。

🔹 核心流程圖
         [Camera] + [Robot State]
                  ↓
         ┌──────────────────────┐
         │  Gr00t Inference AI │ ←── [Lang Instruction]
         └──────────────────────┘
                  ↓
        [Action Chunk] → [SO100Robot] → [Execute]


✅ 總結：
這支程式讓 SO100 機械臂能夠：

用 AI Policy（語言指令驅動）進行操作

用資料集重播模式（模仿示範動作）執行任務

看完後你應該能理解：

SO100Robot = 真實機械臂的硬體 API

Gr00tRobotInferenceClient = 和 AI 推理伺服器對接的介面

main = 選擇要跑「AI 控制」還是「資料集重播」
