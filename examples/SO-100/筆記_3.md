## OpenAI 回答

好的 👍 我幫你逐段解釋這個 eval_gr00t_so100.py 程式的設計與功能。它主要是 NVIDIA GR00T 框架中機械手臂策略推理的評估腳本，支援 so100、so101 機械手臂，並透過語言指令 + 感測資料，來執行動作。

🔑 程式核心功能

讀取機械手臂（so100/so101）的即時感測資料（相機 + 關節狀態）

將觀測資料與語言任務指令丟到「策略推理服務」

接收策略服務的「動作序列 (action chunk)」

逐步將動作送到機械手臂，完成任務

📂 程式結構解析
1. 引入模組
import draccus
import matplotlib.pyplot as plt
import numpy as np
from lerobot.robots import make_robot_from_config
from service import ExternalRobotInferenceClient


draccus：命令列參數管理（像 argparse 升級版）。

matplotlib：用來顯示相機畫面。

lerobot：NVIDIA 提供的機械人控制套件，支援 so100 / so101。

ExternalRobotInferenceClient：與外部 策略推理伺服器（GR00T Policy Server）溝通。

2. Gr00tRobotInferenceClient
class Gr00tRobotInferenceClient:
    def __init__(self, host="localhost", port=5555, camera_keys=[], robot_state_keys=[], show_images=False):
        self.policy = ExternalRobotInferenceClient(host=host, port=port)
        self.camera_keys = camera_keys
        self.robot_state_keys = robot_state_keys
        self.show_images = show_images
        self.modality_keys = ["single_arm", "gripper"]


初始化一個策略客戶端，用來把「機器人觀測資料」傳給伺服器，並拿回「動作計畫」。

camera_keys：相機名稱（如 wrist、front）。

robot_state_keys：關節狀態名稱（6 個 motor state）。

show_images：是否顯示影像。

3. get_action()
def get_action(self, observation_dict, lang: str):
    obs_dict = {f"video.{key}": observation_dict[key] for key in self.camera_keys}
    state = np.array([observation_dict[k] for k in self.robot_state_keys])
    obs_dict["state.single_arm"] = state[:5].astype(np.float64)
    obs_dict["state.gripper"] = state[5:6].astype(np.float64)
    obs_dict["annotation.human.task_description"] = lang


這裡做了幾件事：

把 相機畫面加到輸入。

把 關節狀態（5 個手臂自由度 + 1 個夾爪）加入。

加入 語言指令（例如「Grab pens and place into pen holder」）。

傳給 self.policy.get_action()，取得一組 動作序列 (action chunk)。

回傳的是 一系列動作字典：

[
  {"shoulder_pan.pos": 0.1, "elbow_flex.pos": -0.2, "gripper.pos": 0.5, ...},
  {"shoulder_pan.pos": 0.15, "elbow_flex.pos": -0.25, "gripper.pos": 0.55, ...},
  ...
]

4. view_img()
def view_img(img, overlay_img=None):
    if isinstance(img, dict):
        img = np.concatenate([img[k] for k in img], axis=1)
    plt.imshow(img)
    plt.title("Camera View")
    plt.axis("off")
    plt.pause(0.001)
    plt.clf()


用 matplotlib 即時顯示相機畫面（比 cv2.imshow 穩定）。

5. EvalConfig
@dataclass
class EvalConfig:
    robot: RobotConfig
    policy_host: str = "localhost"
    policy_port: int = 5555
    action_horizon: int = 8
    lang_instruction: str = "Grab pens and place into pen holder."
    play_sounds: bool = False
    timeout: int = 60
    show_images: bool = False


這是一個 參數設定類別，可從命令列傳入：

robot：機器人型號 & 設定（so100/so101）。

policy_host/port：GR00T 策略伺服器位置。

lang_instruction：語言任務指令。

action_horizon：一次要執行幾步動作。

6. eval() 主流程
@draccus.wrap()
def eval(cfg: EvalConfig):
    robot = make_robot_from_config(cfg.robot)
    robot.connect()

    camera_keys = list(cfg.robot.cameras.keys())
    robot_state_keys = list(robot._motors_ft.keys())

    policy = Gr00tRobotInferenceClient(
        host=cfg.policy_host,
        port=cfg.policy_port,
        camera_keys=camera_keys,
        robot_state_keys=robot_state_keys,
    )


建立機器人實例 → 連接硬體。

取得相機 & 馬達 keys。

建立策略客戶端（與伺服器溝通）。

7. Eval Loop
while True:
    observation_dict = robot.get_observation()
    action_chunk = policy.get_action(observation_dict, language_instruction)

    for i in range(cfg.action_horizon):
        action_dict = action_chunk[i]
        robot.send_action(action_dict)
        time.sleep(0.02)


持續迴圈：

取得當前觀測（影像 + 狀態）。

送到策略伺服器 → 拿到動作序列。

依序將動作送到機器人執行。

📝 總結

這個程式的核心作用是：

橋接 機器人硬體 (so100/so101) ↔ 策略推理伺服器 (GR00T Policy Server)。

接收相機 + 狀態資料，配合語言指令 → 得到動作計畫 → 控制機器人執行任務。

適合用來做 語言驅動的機械手臂任務測試（例如：撿起筆 → 放到筆筒）。

